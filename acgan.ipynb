{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time, os, pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class chestxray():\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.transform = transform\n",
    "        self.frame = pd.read_csv(csv_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name= self.frame.iloc[idx, 0]\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        label = self.frame.iloc[idx, 1]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "batch_size = 16\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 256\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 128\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 50000\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "class_num = 3\n",
    "\n",
    "device = torch.device(\"cuda:2\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide which device we want to run on\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "normalize = transforms.Normalize(mean=[0.456, 0.456, 0.456], std=[0.225, 0.225, 0.225])\n",
    "transform = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(256), transforms.ToTensor(), normalize])\n",
    "dataset = chestxray('../training_gans.csv',transform=transform)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = 1\n",
    "        self.label_emb = nn.Embedding(3,3)\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz + 3, ngf * 16, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 16),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*16) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 16 x 16 \n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 64 x 64\n",
    "            nn.ConvTranspose2d(    ngf,      int(ngf/2) , 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(int(ngf/2)),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf/2) x 128 x 128\n",
    "            nn.ConvTranspose2d(    int(ngf/2) ,   nc , 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 256 x 256\n",
    "        )\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        z = input.view(input.size(0), nz, 1, 1)\n",
    "        c = self.label_emb(labels).view(input.size(0), 3, 1, 1)\n",
    "        x = torch.cat([z, c], 1)\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.dnet= original_model\n",
    "        self.fc = nn.Linear(1000, 100)\n",
    "        self.dc = nn.Sequential(\n",
    "            nn.Linear(100, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.cl = nn.Sequential(\n",
    "            nn.Linear(100, 3),\n",
    "        )\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        x = self.dnet(inp)\n",
    "        x = x.view(x.shape[0], 1, x.shape[1])\n",
    "        x = self.fc(x)\n",
    "        d = self.dc(x)\n",
    "        c = self.cl(x)\n",
    "\n",
    "        return d, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (label_emb): Embedding(3, 3)\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(131, 1024, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "    (9): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace)\n",
      "    (12): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU(inplace)\n",
      "    (15): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (16): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): ReLU(inplace)\n",
      "    (18): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (19): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "        \n",
    "# Create the generator\n",
    "netG = Generator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (dnet): DenseNet(\n",
      "    (features): Sequential(\n",
      "      (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu0): ReLU(inplace)\n",
      "      (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (denseblock1): _DenseBlock(\n",
      "        (denselayer1): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer2): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer3): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer4): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer5): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer6): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (transition1): _Transition(\n",
      "        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      )\n",
      "      (denseblock2): _DenseBlock(\n",
      "        (denselayer1): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer2): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer3): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer4): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer5): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer6): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer7): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer8): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer9): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer10): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer11): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer12): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (transition2): _Transition(\n",
      "        (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      )\n",
      "      (denseblock3): _DenseBlock(\n",
      "        (denselayer1): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer2): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer3): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer4): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer5): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer6): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer7): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer8): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer9): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer10): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer11): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer12): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer13): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer14): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer15): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer16): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer17): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer18): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer19): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer20): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer21): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer22): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer23): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer24): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (transition3): _Transition(\n",
      "        (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      )\n",
      "      (denseblock4): _DenseBlock(\n",
      "        (denselayer1): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer2): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer3): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer4): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer5): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer6): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer7): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer8): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer9): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer10): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer11): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer12): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer13): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer14): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer15): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer16): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace)\n",
      "          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (classifier): Linear(in_features=1024, out_features=1000, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (dc): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (cl): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the Discriminator\n",
    "cnn = torchvision.models.densenet121(pretrained=True)\n",
    "netD = Discriminator(cnn).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
    "\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (label_emb): Embedding(3, 3)\n",
       "  (main): Sequential(\n",
       "    (0): ConvTranspose2d(131, 1024, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace)\n",
       "    (9): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU(inplace)\n",
       "    (12): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): ReLU(inplace)\n",
       "    (15): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (16): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (17): ReLU(inplace)\n",
       "    (18): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (19): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netG = Generator(ngpu)\n",
    "netG.load_state_dict(torch.load(\"generator_d_model\",map_location=\"cuda:2\"))\n",
    "netG.to(device)\n",
    "#netD = torch.load(\"discriminator_d\")\n",
    "#netD.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import random\n",
    "# Initialize BCELoss function\n",
    "BCE_loss = nn.BCELoss().to(device)\n",
    "CE_loss= nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = random.uniform(0, 0.2)\n",
    "fake_label = random.uniform(0.8, 1)\n",
    "\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "for param in netD.parameters():\n",
    "     param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/eerun/Python-3.7.2/lib/python3.7/site-packages/torch/nn/modules/loss.py:512: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/50000][0/2231]\tLoss_D: 3.5556\tLoss_G: 1.2471\tD(x): 0.4594\tD(G(z)): 0.4596 / 0.6110\n",
      "[0/50000][50/2231]\tLoss_D: 1.8744\tLoss_G: 2.2666\tD(x): 0.1727\tD(G(z)): 0.8532 / 0.8269\n",
      "[0/50000][100/2231]\tLoss_D: 1.2286\tLoss_G: 4.5108\tD(x): 0.0294\tD(G(z)): 0.9576 / 0.9767\n",
      "[0/50000][150/2231]\tLoss_D: 1.2841\tLoss_G: 5.5495\tD(x): 0.0198\tD(G(z)): 0.9098 / 0.9864\n",
      "[0/50000][200/2231]\tLoss_D: 1.1628\tLoss_G: 5.4216\tD(x): 0.0159\tD(G(z)): 0.9169 / 0.9750\n",
      "[0/50000][250/2231]\tLoss_D: 1.4382\tLoss_G: 4.2369\tD(x): 0.0306\tD(G(z)): 0.9488 / 0.9845\n",
      "[0/50000][300/2231]\tLoss_D: 1.6775\tLoss_G: 4.1777\tD(x): 0.0132\tD(G(z)): 0.9509 / 0.9771\n",
      "[0/50000][350/2231]\tLoss_D: 1.2345\tLoss_G: 3.1709\tD(x): 0.0134\tD(G(z)): 0.8513 / 0.9490\n",
      "[0/50000][400/2231]\tLoss_D: 1.3611\tLoss_G: 4.0259\tD(x): 0.0169\tD(G(z)): 0.9747 / 0.9814\n",
      "[0/50000][450/2231]\tLoss_D: 1.5394\tLoss_G: 3.7819\tD(x): 0.0255\tD(G(z)): 0.9587 / 0.9711\n",
      "[0/50000][500/2231]\tLoss_D: 1.1796\tLoss_G: 4.1129\tD(x): 0.0185\tD(G(z)): 0.9813 / 0.9807\n",
      "[0/50000][550/2231]\tLoss_D: 1.3792\tLoss_G: 4.3191\tD(x): 0.0115\tD(G(z)): 0.9820 / 0.9868\n",
      "[0/50000][600/2231]\tLoss_D: 1.1076\tLoss_G: 3.9526\tD(x): 0.0126\tD(G(z)): 0.9754 / 0.9810\n",
      "[0/50000][650/2231]\tLoss_D: 1.5244\tLoss_G: 4.2400\tD(x): 0.0108\tD(G(z)): 0.9885 / 0.9856\n",
      "[0/50000][700/2231]\tLoss_D: 0.9922\tLoss_G: 3.9689\tD(x): 0.0127\tD(G(z)): 0.9800 / 0.9804\n",
      "[0/50000][750/2231]\tLoss_D: 1.3611\tLoss_G: 4.2253\tD(x): 0.0128\tD(G(z)): 0.9737 / 0.9822\n",
      "[0/50000][800/2231]\tLoss_D: 1.0978\tLoss_G: 4.1768\tD(x): 0.0103\tD(G(z)): 0.9733 / 0.9811\n",
      "[0/50000][850/2231]\tLoss_D: 1.1216\tLoss_G: 4.0184\tD(x): 0.0112\tD(G(z)): 0.9744 / 0.9818\n",
      "[0/50000][900/2231]\tLoss_D: 1.1345\tLoss_G: 3.9200\tD(x): 0.0090\tD(G(z)): 0.9786 / 0.9788\n",
      "[0/50000][950/2231]\tLoss_D: 1.5106\tLoss_G: 8.9255\tD(x): 0.0128\tD(G(z)): 0.9960 / 0.9993\n",
      "[0/50000][1000/2231]\tLoss_D: 1.5756\tLoss_G: 5.3637\tD(x): 0.0098\tD(G(z)): 0.9962 / 0.9933\n",
      "[0/50000][1050/2231]\tLoss_D: 1.1794\tLoss_G: 3.7621\tD(x): 0.0113\tD(G(z)): 0.9832 / 0.9745\n",
      "[0/50000][1100/2231]\tLoss_D: 1.2046\tLoss_G: 3.9013\tD(x): 0.0113\tD(G(z)): 0.9667 / 0.9799\n",
      "[0/50000][1150/2231]\tLoss_D: 0.8670\tLoss_G: 4.0180\tD(x): 0.0099\tD(G(z)): 0.9766 / 0.9825\n",
      "[0/50000][1200/2231]\tLoss_D: 1.0952\tLoss_G: 3.9331\tD(x): 0.0121\tD(G(z)): 0.9807 / 0.9808\n",
      "[0/50000][1250/2231]\tLoss_D: 1.0336\tLoss_G: 3.9774\tD(x): 0.0122\tD(G(z)): 0.9723 / 0.9817\n",
      "[0/50000][1300/2231]\tLoss_D: 1.3084\tLoss_G: 4.7682\tD(x): 0.0144\tD(G(z)): 0.9955 / 0.9911\n",
      "[0/50000][1350/2231]\tLoss_D: 1.0223\tLoss_G: 3.8980\tD(x): 0.0095\tD(G(z)): 0.9790 / 0.9802\n",
      "[0/50000][1400/2231]\tLoss_D: 0.9782\tLoss_G: 3.8051\tD(x): 0.0132\tD(G(z)): 0.9772 / 0.9784\n",
      "[0/50000][1450/2231]\tLoss_D: 1.1789\tLoss_G: 4.1083\tD(x): 0.0097\tD(G(z)): 0.9832 / 0.9838\n",
      "[0/50000][1500/2231]\tLoss_D: 1.2085\tLoss_G: 3.9581\tD(x): 0.0141\tD(G(z)): 0.9789 / 0.9814\n",
      "[0/50000][1550/2231]\tLoss_D: 0.9632\tLoss_G: 3.7756\tD(x): 0.0114\tD(G(z)): 0.9705 / 0.9776\n",
      "[0/50000][1600/2231]\tLoss_D: 1.1810\tLoss_G: 3.8919\tD(x): 0.0118\tD(G(z)): 0.9771 / 0.9801\n",
      "[0/50000][1650/2231]\tLoss_D: 1.0054\tLoss_G: 3.7279\tD(x): 0.0116\tD(G(z)): 0.9802 / 0.9766\n",
      "[0/50000][1700/2231]\tLoss_D: 1.0342\tLoss_G: 3.7421\tD(x): 0.0114\tD(G(z)): 0.9833 / 0.9762\n",
      "[0/50000][1750/2231]\tLoss_D: 1.1631\tLoss_G: 4.0492\tD(x): 0.0059\tD(G(z)): 0.9835 / 0.9828\n",
      "[0/50000][1800/2231]\tLoss_D: 1.0787\tLoss_G: 4.0973\tD(x): 0.0102\tD(G(z)): 0.9828 / 0.9834\n",
      "[0/50000][1850/2231]\tLoss_D: 1.2690\tLoss_G: 3.7069\tD(x): 0.0212\tD(G(z)): 0.9913 / 0.9753\n",
      "[0/50000][1900/2231]\tLoss_D: 0.9059\tLoss_G: 3.6090\tD(x): 0.0145\tD(G(z)): 0.9841 / 0.9737\n",
      "[0/50000][1950/2231]\tLoss_D: 1.2271\tLoss_G: 4.0581\tD(x): 0.0117\tD(G(z)): 0.9830 / 0.9827\n",
      "[0/50000][2000/2231]\tLoss_D: 0.8186\tLoss_G: 3.9506\tD(x): 0.0103\tD(G(z)): 0.9843 / 0.9805\n",
      "[0/50000][2050/2231]\tLoss_D: 1.2771\tLoss_G: 4.4094\tD(x): 0.0117\tD(G(z)): 0.9892 / 0.9879\n",
      "[0/50000][2100/2231]\tLoss_D: 1.0913\tLoss_G: 4.0716\tD(x): 0.0103\tD(G(z)): 0.9818 / 0.9817\n",
      "[0/50000][2150/2231]\tLoss_D: 1.2120\tLoss_G: 3.9631\tD(x): 0.0113\tD(G(z)): 0.9809 / 0.9813\n",
      "[0/50000][2200/2231]\tLoss_D: 1.1909\tLoss_G: 3.8589\tD(x): 0.0139\tD(G(z)): 0.9845 / 0.9796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/eerun/Python-3.7.2/lib/python3.7/site-packages/torch/nn/modules/loss.py:512: UserWarning: Using a target size (torch.Size([12])) that is different to the input size (torch.Size([12, 1, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "/eerun/Python-3.7.2/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Discriminator. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/50000][0/2231]\tLoss_D: 1.1943\tLoss_G: 3.9652\tD(x): 0.0144\tD(G(z)): 0.9671 / 0.9809\n",
      "[1/50000][50/2231]\tLoss_D: 1.4121\tLoss_G: 3.8170\tD(x): 0.0090\tD(G(z)): 0.9768 / 0.9782\n",
      "[1/50000][100/2231]\tLoss_D: 1.1746\tLoss_G: 4.0227\tD(x): 0.0130\tD(G(z)): 0.9832 / 0.9826\n",
      "[1/50000][150/2231]\tLoss_D: 1.1071\tLoss_G: 3.8177\tD(x): 0.0097\tD(G(z)): 0.9809 / 0.9785\n",
      "[1/50000][200/2231]\tLoss_D: 1.3351\tLoss_G: 4.2301\tD(x): 0.0085\tD(G(z)): 0.9772 / 0.9836\n",
      "[1/50000][250/2231]\tLoss_D: 1.2035\tLoss_G: 5.1622\tD(x): 0.0076\tD(G(z)): 0.9809 / 0.9591\n",
      "[1/50000][300/2231]\tLoss_D: 1.3137\tLoss_G: 3.5239\tD(x): 0.0878\tD(G(z)): 0.9897 / 0.9647\n",
      "[1/50000][350/2231]\tLoss_D: 1.2533\tLoss_G: 3.5971\tD(x): 0.0112\tD(G(z)): 0.9833 / 0.9733\n",
      "[1/50000][400/2231]\tLoss_D: 1.2124\tLoss_G: 4.3507\tD(x): 0.0039\tD(G(z)): 0.9759 / 0.9864\n",
      "[1/50000][450/2231]\tLoss_D: 1.1940\tLoss_G: 3.9722\tD(x): 0.0158\tD(G(z)): 0.9774 / 0.9815\n",
      "[1/50000][500/2231]\tLoss_D: 1.0509\tLoss_G: 4.4398\tD(x): 0.0109\tD(G(z)): 0.9888 / 0.9881\n",
      "[1/50000][550/2231]\tLoss_D: 1.0063\tLoss_G: 3.8325\tD(x): 0.0080\tD(G(z)): 0.9743 / 0.9790\n",
      "[1/50000][600/2231]\tLoss_D: 1.0794\tLoss_G: 3.9282\tD(x): 0.0178\tD(G(z)): 0.9825 / 0.9807\n",
      "[1/50000][650/2231]\tLoss_D: 0.8489\tLoss_G: 3.8723\tD(x): 0.0087\tD(G(z)): 0.9784 / 0.9797\n",
      "[1/50000][700/2231]\tLoss_D: 1.0683\tLoss_G: 3.9622\tD(x): 0.0094\tD(G(z)): 0.9845 / 0.9797\n",
      "[1/50000][750/2231]\tLoss_D: 1.1216\tLoss_G: 3.7056\tD(x): 0.0112\tD(G(z)): 0.9710 / 0.9761\n",
      "[1/50000][800/2231]\tLoss_D: 1.0699\tLoss_G: 3.9232\tD(x): 0.0067\tD(G(z)): 0.9795 / 0.9808\n",
      "[1/50000][850/2231]\tLoss_D: 1.1234\tLoss_G: 3.8152\tD(x): 0.0082\tD(G(z)): 0.9774 / 0.9786\n",
      "[1/50000][900/2231]\tLoss_D: 1.2551\tLoss_G: 3.7370\tD(x): 0.0163\tD(G(z)): 0.9782 / 0.9768\n",
      "[1/50000][950/2231]\tLoss_D: 0.9731\tLoss_G: 3.8138\tD(x): 0.0117\tD(G(z)): 0.9746 / 0.9780\n",
      "[1/50000][1000/2231]\tLoss_D: 0.8894\tLoss_G: 3.7096\tD(x): 0.0105\tD(G(z)): 0.9718 / 0.9761\n",
      "[1/50000][1050/2231]\tLoss_D: 0.9503\tLoss_G: 3.9113\tD(x): 0.0092\tD(G(z)): 0.9770 / 0.9795\n",
      "[1/50000][1100/2231]\tLoss_D: 1.3868\tLoss_G: 3.6321\tD(x): 0.0221\tD(G(z)): 0.9762 / 0.9735\n",
      "[1/50000][1150/2231]\tLoss_D: 1.1277\tLoss_G: 4.2637\tD(x): 0.0082\tD(G(z)): 0.9867 / 0.9854\n",
      "[1/50000][1200/2231]\tLoss_D: 1.2488\tLoss_G: 4.0304\tD(x): 0.0096\tD(G(z)): 0.9817 / 0.9825\n",
      "[1/50000][1250/2231]\tLoss_D: 1.1444\tLoss_G: 4.2576\tD(x): 0.0032\tD(G(z)): 0.9804 / 0.9859\n",
      "[1/50000][1300/2231]\tLoss_D: 0.9857\tLoss_G: 3.7306\tD(x): 0.0084\tD(G(z)): 0.9742 / 0.9767\n",
      "[1/50000][1350/2231]\tLoss_D: 1.2243\tLoss_G: 3.8412\tD(x): 0.0084\tD(G(z)): 0.9771 / 0.9787\n",
      "[1/50000][1400/2231]\tLoss_D: 1.2304\tLoss_G: 3.8492\tD(x): 0.0107\tD(G(z)): 0.9702 / 0.9789\n",
      "[1/50000][1450/2231]\tLoss_D: 1.2569\tLoss_G: 3.7765\tD(x): 0.0100\tD(G(z)): 0.9740 / 0.9776\n",
      "[1/50000][1500/2231]\tLoss_D: 1.1099\tLoss_G: 3.9713\tD(x): 0.0093\tD(G(z)): 0.9826 / 0.9816\n",
      "[1/50000][1550/2231]\tLoss_D: 1.0526\tLoss_G: 4.3327\tD(x): 0.0128\tD(G(z)): 0.9761 / 0.9871\n",
      "[1/50000][1600/2231]\tLoss_D: 1.2850\tLoss_G: 4.6655\tD(x): 0.0091\tD(G(z)): 0.9942 / 0.9905\n",
      "[1/50000][1650/2231]\tLoss_D: 1.8568\tLoss_G: 4.8775\tD(x): 0.0114\tD(G(z)): 0.9751 / 0.9926\n",
      "[1/50000][1700/2231]\tLoss_D: 1.2851\tLoss_G: 3.8033\tD(x): 0.0219\tD(G(z)): 0.9705 / 0.9764\n",
      "[1/50000][1750/2231]\tLoss_D: 0.9937\tLoss_G: 4.8197\tD(x): 0.0039\tD(G(z)): 0.9725 / 0.9920\n",
      "[1/50000][1800/2231]\tLoss_D: 1.1108\tLoss_G: 3.8661\tD(x): 0.0111\tD(G(z)): 0.9753 / 0.9795\n",
      "[1/50000][1850/2231]\tLoss_D: 1.2776\tLoss_G: 3.9166\tD(x): 0.0116\tD(G(z)): 0.9786 / 0.9807\n",
      "[1/50000][1900/2231]\tLoss_D: 1.2455\tLoss_G: 3.6706\tD(x): 0.0110\tD(G(z)): 0.9728 / 0.9745\n",
      "[1/50000][1950/2231]\tLoss_D: 1.2961\tLoss_G: 4.0260\tD(x): 0.0095\tD(G(z)): 0.9826 / 0.9826\n",
      "[1/50000][2000/2231]\tLoss_D: 1.1145\tLoss_G: 3.8432\tD(x): 0.0063\tD(G(z)): 0.9842 / 0.9789\n",
      "[1/50000][2050/2231]\tLoss_D: 1.3378\tLoss_G: 3.8346\tD(x): 0.0074\tD(G(z)): 0.9822 / 0.9787\n",
      "[1/50000][2100/2231]\tLoss_D: 1.1290\tLoss_G: 3.8482\tD(x): 0.0130\tD(G(z)): 0.9776 / 0.9791\n",
      "[1/50000][2150/2231]\tLoss_D: 0.7563\tLoss_G: 3.8369\tD(x): 0.0104\tD(G(z)): 0.9747 / 0.9787\n",
      "[1/50000][2200/2231]\tLoss_D: 1.3353\tLoss_G: 4.0274\tD(x): 0.0119\tD(G(z)): 0.9784 / 0.9827\n",
      "[2/50000][0/2231]\tLoss_D: 1.3032\tLoss_G: 3.7507\tD(x): 0.0085\tD(G(z)): 0.9759 / 0.9768\n",
      "[2/50000][50/2231]\tLoss_D: 0.9685\tLoss_G: 3.8824\tD(x): 0.0082\tD(G(z)): 0.9766 / 0.9800\n",
      "[2/50000][100/2231]\tLoss_D: 1.2840\tLoss_G: 3.7833\tD(x): 0.0094\tD(G(z)): 0.9768 / 0.9776\n",
      "[2/50000][150/2231]\tLoss_D: 1.3009\tLoss_G: 3.9602\tD(x): 0.0109\tD(G(z)): 0.9761 / 0.9814\n",
      "[2/50000][200/2231]\tLoss_D: 1.1049\tLoss_G: 3.8273\tD(x): 0.0099\tD(G(z)): 0.9777 / 0.9788\n",
      "[2/50000][250/2231]\tLoss_D: 1.0081\tLoss_G: 4.0391\tD(x): 0.0097\tD(G(z)): 0.9841 / 0.9821\n",
      "[2/50000][300/2231]\tLoss_D: 1.4357\tLoss_G: 4.0543\tD(x): 0.0070\tD(G(z)): 0.9836 / 0.9829\n",
      "[2/50000][350/2231]\tLoss_D: 1.0702\tLoss_G: 3.9059\tD(x): 0.0102\tD(G(z)): 0.9802 / 0.9805\n",
      "[2/50000][400/2231]\tLoss_D: 0.9434\tLoss_G: 3.9410\tD(x): 0.0158\tD(G(z)): 0.9815 / 0.9811\n",
      "[2/50000][450/2231]\tLoss_D: 1.0947\tLoss_G: 3.8627\tD(x): 0.0125\tD(G(z)): 0.9782 / 0.9795\n",
      "[2/50000][500/2231]\tLoss_D: 1.0043\tLoss_G: 3.8141\tD(x): 0.0091\tD(G(z)): 0.9769 / 0.9783\n",
      "[2/50000][550/2231]\tLoss_D: 1.1935\tLoss_G: 3.7442\tD(x): 0.0102\tD(G(z)): 0.9787 / 0.9768\n",
      "[2/50000][600/2231]\tLoss_D: 1.2552\tLoss_G: 3.8991\tD(x): 0.0108\tD(G(z)): 0.9777 / 0.9803\n",
      "[2/50000][650/2231]\tLoss_D: 1.0544\tLoss_G: 3.8675\tD(x): 0.0102\tD(G(z)): 0.9796 / 0.9797\n",
      "[2/50000][700/2231]\tLoss_D: 0.9234\tLoss_G: 3.8158\tD(x): 0.0127\tD(G(z)): 0.9747 / 0.9786\n",
      "[2/50000][750/2231]\tLoss_D: 1.1404\tLoss_G: 4.1417\tD(x): 0.0107\tD(G(z)): 0.9815 / 0.9844\n",
      "[2/50000][800/2231]\tLoss_D: 1.0095\tLoss_G: 3.9058\tD(x): 0.0092\tD(G(z)): 0.9732 / 0.9801\n",
      "[2/50000][850/2231]\tLoss_D: 1.1123\tLoss_G: 3.9460\tD(x): 0.0106\tD(G(z)): 0.9811 / 0.9809\n",
      "[2/50000][900/2231]\tLoss_D: 1.1150\tLoss_G: 4.2931\tD(x): 0.0090\tD(G(z)): 0.9878 / 0.9866\n",
      "[2/50000][950/2231]\tLoss_D: 1.2735\tLoss_G: 3.6985\tD(x): 0.0126\tD(G(z)): 0.9747 / 0.9759\n",
      "[2/50000][1000/2231]\tLoss_D: 1.0439\tLoss_G: 3.8845\tD(x): 0.0110\tD(G(z)): 0.9768 / 0.9800\n",
      "[2/50000][1050/2231]\tLoss_D: 1.2339\tLoss_G: 3.2154\tD(x): 0.0017\tD(G(z)): 0.9525 / 0.9576\n",
      "[2/50000][1100/2231]\tLoss_D: 1.3112\tLoss_G: 12.8206\tD(x): 0.0132\tD(G(z)): 0.9932 / 0.9870\n",
      "[2/50000][1150/2231]\tLoss_D: 1.3720\tLoss_G: 11.6014\tD(x): 0.0069\tD(G(z)): 0.9781 / 0.9920\n",
      "[2/50000][1200/2231]\tLoss_D: 1.9038\tLoss_G: 12.2944\tD(x): 0.0143\tD(G(z)): 0.9949 / 0.9971\n",
      "[2/50000][1250/2231]\tLoss_D: 1.5236\tLoss_G: 14.8217\tD(x): 0.0118\tD(G(z)): 0.9665 / 0.9963\n",
      "[2/50000][1300/2231]\tLoss_D: 1.3951\tLoss_G: 9.5370\tD(x): 0.0039\tD(G(z)): 0.9869 / 0.9912\n",
      "[2/50000][1350/2231]\tLoss_D: 1.0544\tLoss_G: 3.6847\tD(x): 0.0106\tD(G(z)): 0.9607 / 0.9750\n",
      "[2/50000][1400/2231]\tLoss_D: 1.0573\tLoss_G: 3.9981\tD(x): 0.0055\tD(G(z)): 0.9797 / 0.9821\n",
      "[2/50000][1450/2231]\tLoss_D: 0.8801\tLoss_G: 4.1592\tD(x): 0.0154\tD(G(z)): 0.9841 / 0.9847\n",
      "[2/50000][1500/2231]\tLoss_D: 1.1035\tLoss_G: 3.7473\tD(x): 0.0129\tD(G(z)): 0.9765 / 0.9770\n",
      "[2/50000][1550/2231]\tLoss_D: 1.1618\tLoss_G: 3.9586\tD(x): 0.0141\tD(G(z)): 0.9726 / 0.9815\n",
      "[2/50000][1600/2231]\tLoss_D: 0.7593\tLoss_G: 3.8096\tD(x): 0.0098\tD(G(z)): 0.9768 / 0.9785\n",
      "[2/50000][1650/2231]\tLoss_D: 1.1126\tLoss_G: 4.2353\tD(x): 0.0108\tD(G(z)): 0.9869 / 0.9858\n",
      "[2/50000][1700/2231]\tLoss_D: 0.8501\tLoss_G: 3.9943\tD(x): 0.0089\tD(G(z)): 0.9830 / 0.9819\n",
      "[2/50000][1750/2231]\tLoss_D: 1.3682\tLoss_G: 3.7974\tD(x): 0.0086\tD(G(z)): 0.9782 / 0.9782\n",
      "[2/50000][1800/2231]\tLoss_D: 1.0314\tLoss_G: 3.8014\tD(x): 0.0066\tD(G(z)): 0.9724 / 0.9784\n",
      "[2/50000][1850/2231]\tLoss_D: 1.0292\tLoss_G: 3.9735\tD(x): 0.0134\tD(G(z)): 0.9817 / 0.9817\n",
      "[2/50000][1900/2231]\tLoss_D: 1.0819\tLoss_G: 3.7518\tD(x): 0.0090\tD(G(z)): 0.9756 / 0.9771\n",
      "[2/50000][1950/2231]\tLoss_D: 1.0565\tLoss_G: 3.9982\tD(x): 0.0088\tD(G(z)): 0.9803 / 0.9821\n",
      "[2/50000][2000/2231]\tLoss_D: 1.5500\tLoss_G: 4.1794\tD(x): 0.0079\tD(G(z)): 0.9846 / 0.9847\n",
      "[2/50000][2050/2231]\tLoss_D: 1.1156\tLoss_G: 4.0470\tD(x): 0.0101\tD(G(z)): 0.9824 / 0.9831\n",
      "[2/50000][2100/2231]\tLoss_D: 0.9727\tLoss_G: 3.7347\tD(x): 0.0054\tD(G(z)): 0.9751 / 0.9767\n",
      "[2/50000][2150/2231]\tLoss_D: 1.1311\tLoss_G: 3.8675\tD(x): 0.0124\tD(G(z)): 0.9775 / 0.9789\n",
      "[2/50000][2200/2231]\tLoss_D: 0.9526\tLoss_G: 3.8667\tD(x): 0.0082\tD(G(z)): 0.9793 / 0.9795\n",
      "[3/50000][0/2231]\tLoss_D: 1.1714\tLoss_G: 3.8056\tD(x): 0.0053\tD(G(z)): 0.9753 / 0.9781\n",
      "[3/50000][50/2231]\tLoss_D: 0.9090\tLoss_G: 3.8570\tD(x): 0.0120\tD(G(z)): 0.9809 / 0.9795\n",
      "[3/50000][100/2231]\tLoss_D: 1.3646\tLoss_G: 3.9955\tD(x): 0.0110\tD(G(z)): 0.9840 / 0.9822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/50000][150/2231]\tLoss_D: 1.1377\tLoss_G: 3.8428\tD(x): 0.0076\tD(G(z)): 0.9775 / 0.9790\n",
      "[3/50000][200/2231]\tLoss_D: 1.0114\tLoss_G: 3.9806\tD(x): 0.0115\tD(G(z)): 0.9832 / 0.9819\n",
      "[3/50000][250/2231]\tLoss_D: 1.3167\tLoss_G: 3.9666\tD(x): 0.0075\tD(G(z)): 0.9764 / 0.9813\n",
      "[3/50000][300/2231]\tLoss_D: 1.1488\tLoss_G: 3.8758\tD(x): 0.0078\tD(G(z)): 0.9802 / 0.9799\n",
      "[3/50000][350/2231]\tLoss_D: 1.0846\tLoss_G: 3.8268\tD(x): 0.0087\tD(G(z)): 0.9749 / 0.9789\n",
      "[3/50000][400/2231]\tLoss_D: 0.9838\tLoss_G: 3.8359\tD(x): 0.0097\tD(G(z)): 0.9763 / 0.9790\n",
      "[3/50000][450/2231]\tLoss_D: 1.0629\tLoss_G: 3.8695\tD(x): 0.0097\tD(G(z)): 0.9714 / 0.9798\n",
      "[3/50000][500/2231]\tLoss_D: 1.0183\tLoss_G: 3.8693\tD(x): 0.0073\tD(G(z)): 0.9767 / 0.9797\n",
      "[3/50000][550/2231]\tLoss_D: 1.0857\tLoss_G: 3.8212\tD(x): 0.0083\tD(G(z)): 0.9774 / 0.9787\n",
      "[3/50000][600/2231]\tLoss_D: 1.0396\tLoss_G: 3.8357\tD(x): 0.0110\tD(G(z)): 0.9793 / 0.9789\n",
      "[3/50000][650/2231]\tLoss_D: 1.2375\tLoss_G: 3.7983\tD(x): 0.0084\tD(G(z)): 0.9765 / 0.9783\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, (data, labels) in enumerate(data_loader, 0):\n",
    "        \n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        # Format batch\n",
    "        real_cpu = data.to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, device=device)\n",
    "        # Forward pass real batch through D\n",
    "        output, pred = netD(real_cpu)\n",
    "        #labels= torch.from_numpy(np.squeeze(np.eye(3)[labels.reshape(-1)]) ).to(device)\n",
    "        labels= labels.long().to(device)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = BCE_loss(output, label)\n",
    "        errC_real= CE_loss(pred.squeeze(), labels)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        (errD_real+errC_real).backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = Variable(torch.randn(b_size, nz, 1, 1)).to(device)\n",
    "        fake_labels = Variable(torch.LongTensor(np.random.randint(0, 3, b_size))).to(device)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise, fake_labels)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output, pred = netD(fake.detach())\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = BCE_loss(output, label)\n",
    "        errC_fake= CE_loss(pred.squeeze(), fake_labels)\n",
    "        # Calculate the gradients for this batch\n",
    "        (errD_fake+errC_fake).backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Add the gradients from the all-real and all-fake batches\n",
    "        errD = errD_real + errD_fake +errC_real +errC_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output, pred = netD(fake)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = BCE_loss(output, label)\n",
    "        errG_C= CE_loss(pred.squeeze(), fake_labels)\n",
    "        errG+=errG_C\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "      #  while(errG - errD > 7):\n",
    "       #     print(\"in the loop\")\n",
    "        #    netG.zero_grad()\n",
    "         #   label.fill_(real_label)  # fake labels are real for generator cost\n",
    "            # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        #    output = netD(fake, fake_labels).view(-1)\n",
    "            # Calculate G's loss based on this output\n",
    "        #    errG = criterion(output, label)\n",
    "            # Calculate gradients for G\n",
    "        #    errG.backward(retain_graph=True)\n",
    "        #    D_G_z2 = output.mean().item()\n",
    "            # Update G\n",
    "        #    optimizerG.step()\n",
    "            # Output training stats\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(data_loader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "     #   if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "     #       with torch.no_grad():\n",
    "     #           fake = netG(fixed_noise).detach().cpu()\n",
    "      #      img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1\n",
    "    torch.save(netG.state_dict(), \"generator_d_model\")\n",
    "    #torch.save(netD.state_dict(), \"discriminator_d_model\")\n",
    "    torch.save(netD,\"discriminator_d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
